@workflow-loop input=<optional> clone=<optional> clean_results=<optional>
---
temperature: 0.1
model: gpt-5
---

Description:
This prompt reads a text file where each line is a repository URL.
For each line, it executes a list of tasks defined in the `@tasks` prompt.
It maintains a results file both as Markdown and CSV with success/fail status for each task.

Behavior:
1. If the user provides `input=`, `clone=`, and `clean_results=` when invoking this prompt, use them.
   Defaults:
      input = "repositories.txt"
      clone = "./cloned"
      clean_results = true
2. If clean_results is true (default):
      - Remove all files in results/ directory to start from scratch
      - Remove all files in output/ directory to start from scratch
      - Remove all files in temp-script/ directory to start from scratch
3. Ensure the clone directory exists; create if it does not.

3. Create and initialize a repository progress markdown file:
      - results/repo-progress.md (Repository Progress tracking table)
      - Parse all repository URLs from input file to get friendly repo names
      - Parse repo_tasks_list.md to extract ALL task directive names (e.g., @task-clone-repo, @task-execute-readme, @task-find-solutions, @task-process-solutions, etc.)
      - Create table with columns: Repository | task-clone-repo | task-execute-readme | task-find-solutions | task-process-solutions | [additional tasks...]
      - **IMPORTANT**: Include a column for EVERY task found in repo_tasks_list.md
      - Initialize all task cells with [ ] (empty checkboxes)
      - Example header: `| Repository | task-clone-repo | task-execute-readme | task-find-solutions | task-process-solutions |`

4. Initialize results files:
      - results/repo-results.md (Repository Markdown table)
      - results/repo-results.csv (Repository CSV table)
    
5. Read the text file and process ALL repositories:
   - Read all non-empty, non-comment lines from the input file
   - For EACH repository URL in the file:
      a. Extract a friendly repo_name from the URL
      b. Call `@tasks-list` with repo_url and clone_path
      c. Append success/fail for each task to Markdown + CSV
      d. Update repo-progress.md with task completion checkboxes
   - **CRITICAL**: Process EVERY repository in the input file, not just the first one
   - **IMPLEMENTATION**: Create a master loop script that reads all repository URLs and processes each one sequentially
6. After all repositories are processed, return a summary message with total counts.

Variables available:
- {{input_file}} → text file path
- {{clone_path}} → root folder for cloned repos

Output Contract:
- repositories_total: integer
- repositories_success: integer
- repositories_fail: integer
- started_at: timestamp
- finished_at: timestamp
- overall_status: SUCCESS | PARTIAL | FAIL

Implementation Notes (conceptual):
1. Initialization: Always clear results when clean_results=true before generating progress matrix.
2. Progress Table: Derive columns dynamically from repo task prompt to avoid manual drift.
3. Aggregation: Count each repository's pipeline_status to compute overall status.
4. Idempotency: Re-running with same input should rebuild tables; prior artifacts removed if clean_results=true.
5. Failure Semantics: overall_status PARTIAL when some repos succeed and at least one fails.
6. **Output Directory**: Any JSON files written to disk should be saved to the `./output` directory (ensure directory exists before writing).
7. **Programming Language**: All code generated and executed by this prompt or any prompts triggered by this prompt MUST be written in Python.
8. **Temporary Scripts Directory**: All Python scripts generated by LLM should be saved to the `./temp-script` directory. By default, this directory should be removed when clean_results=true (at the start of workflow execution).
